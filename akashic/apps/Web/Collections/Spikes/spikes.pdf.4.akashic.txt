particular phenomenon might take without a grounding in nonlinear dynamics. Second,  even when the relevant equations are already known, it is very difficult to determine how  solutions depend on parameter values without knowledge of dynamical techniques.  Finally, a knowledge of nonlinear dynamics is required if one is to be certain that one's  computer approximations actually reflect the true dynamics of the system under study.  Having said this, it must certainly be acknowledged that computer simulations provide  an extremely valuable and powerful tool for the neuroscientist. Accordingly, another goal  of this book is to teach precisely those analytical techniques that will enhance the power  and sophistication of neural simulations. To this end I have included numerous computer  simulations of single neurons and small neural networks throughout the book. The goal  of these is both to enable the reader to explore dynamical phenomena that are discussed in  the book and to learn to apply analytical techniques to the understanding of network  simulations. MatLab™ scripts for these simulations are contained on the accompanying  disk. In addition, computers have enhanced the utility of some techniques and removed  the drudgery from others. Accordingly, MatLab scripts are included to produce symbolic  solutions to second order linear differential equations and to facilitate application of  stability criteria to higher order systems. There is no point in doing by hand what a  computer can do faster and more accurately.  Currently, 'connectionist' neural networks are very much in vogue, and a number of  excellent texts already exist in this area (e.g. Hertz et ai, 1991). This book is not intended  to compete with these but rather to complement them. Connectionism today is concerned  with somewhat artificial neuron-like networks that can be trained (by back-propagation,  for example) to associate and categorize stimuli. My emphasis will be on smaller, deter- ministic neural networks that are more biological in their behavior. Furthermore, the  nonlinear analysis in this book provides much of the mathematical background upon  which connectionist neural modeling has been built (see Chapter 14). It has been  my experience that many students who are attracted to connectionist modeling have  strong computer science backgrounds but rather little understanding of the nonlinear  dynamics upon which their modeling is based. I hope that this book will help to rectify the  situation.  1.2 Neuroscience and levels of abstraction  As in any branch of science, there are many different levels of abstraction at which a  neuron or neural network may be described. At the most abstract level, a neuron may be  described as a device that is either on or off (1 or 0). This was the description introduced  by McCulloch and Pitts (1943) in their classic paper: 'A logical calculus of ideas immanent  4  Spikes, decisions, and actions  in nervous activity'. At a more detailed level, a neuron may be described by its spike rate,  which varies continuously between zero (when the postsynaptic potential is below  threshold) and some maximum value at which the spike rate saturates due to the absolute  refractory period. A yet more detailed neural description is provided by the Hodgkin- Huxley (1952) equations, which describe the generation and shape of each individual  action potential as a function of the underlying ionic currents. Finally, the most detailed  descriptions incorporate the detailed geometry of the dendritic tree along with the spatial  distribution of synapses and ion channels on the dendrites. This, of course, is not the end  of the line: one could frame one's neural simulations at the level of quantum mechan- ical changes in ion channel gating molecules. However, contemporary neuroscience  seldom goes so far: neural reductionism generally stops with the belief that quantum  mechanical effects will either average out or else manifest themselves as statistical  fluctuations.  Different levels of generality in the description of neurons are appropriate for different  purposes, as is illustrated in Fig. 1.1. Generally speaking, the more detailed the  description of individual neurons, the smaller the number of neurons that can be effect- ively modeled. As different types of neural experimentation produce data reflecting  activity of widely varying neural populations, however, several different levels of  Levels of Abstraction in Neuroscience  Level  Number of Neurons  Typical Data  Macromolecular  Quantum Mechanics  Individual  Ion Channel  Vastly < 1  Fraction of membrane  _n_ji  n_  Channel opening & closing  Individual  Action Potentials  Neural Spike Rates  PET, fMRI &  Evoked Potentials  1 -10  (1-10,000 for modeling)  1-100  (10-106 for modeling)  Spike trains  QHW  Post stimulus time histogram  105• 109  Activated brain region  Fig. 1.1  Different levels of abstraction used to describe neurons and neural networks. Each level is  applicable to different network size.  Introduction 