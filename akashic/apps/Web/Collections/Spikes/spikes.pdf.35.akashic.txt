20  30  Time (ms)  40  50  Fig. 5.1 Illustration of Euler's method with two different time steps h compared with the exact solution  to (5.5).  62  Spikes, decisions, and actions  method progressively overestimates the exact solution in this case and produces a value  of 11.94 after 40 ms. Compared to the exact value of 10.83, this represents an error of  10%, not too impressive. The situation is improved by reducing the step size to h =1 ms,  which produces an estimate of A =11.10 at 40 ms, an error reduction to 2.5%. Thus, the  approximation has improved by a factor of 4.0 by reducing the time step to a quarter of  the original value, but this improvement is at the cost of having to do calculations for 40  time iterations instead of 10. This is always the issue in approximating the solution to  differential equations: the simulation will become more and more accurate as h is made  smaller, but this multiplies the number of computations and hence the amount of com- puter time for the simulation. Life is too short to use excruciatingly small time steps!  5.2 Runge-Kutta methods  Although Euler's method can be used for simple problems if the step size h is sufficiently  small, it is rarely used in practice, because more accurate results may be obtained with the  same amount of computation using other methods. Furthermore, around a steady state  that is a center, Euler's method is actually guaranteed to produce an unstable spiral  solution regardless of step size h, a phenomenon investigated in the problems. The reason  for the inaccuracy of Euler's method is that (5.4) only uses a straight line approximation  to the solution at each time to obtain the approximation at the next time step, a con- sequence of truncating Taylor's series in (5.3) after the linear term in h. One might guess  that greater accuracy could be obtained by retaining additional terms in the Taylor series  approximation. This guess is correct, and it forms the basis for Runge-Kutta approxi- mation methods. There are several different orders of Runge-Kutta methods, the order  being determined by the highest power retained in the Taylor series approximation. The  fourth order Runge-Kutta method is the most common, and we shall use it for neural  simulations later in this book. To simplify the algebra, however, let us examine the  second order case. In addition, let F(x) in (5.1) be independent of ?, which will simplify  notation and highlight the conceptual aspects of the derivation.  To derive the second order Runge-Kutta approximation, let us first expand the Taylor  series in (5.3) to include the second order term:  .v(?.v + h) « A-(?,V) + l'~ + ~-A  (5-6)  d?  2 d?- Substituting for d.v/d? using (5.1) gives:  x{tN + h) « A(?,V) + hF + — —  (5.7)  This strategy seems promising, but it now becomes necessary to evaluate dF/dt. If Fis a  well-defined mathematical function, the differentiation might be carried out analytically.  (Note that F depends on ? implicitly through its dependence on x(t) even if ? does not  appear explicitly in F.) However, this can lead to very messy expressions, especially when  Approximation and simulation  63  higher order Runge-Kutta methods are considered. For this reason the Runge-Kutta  method employs a very clever trick to avoid any explicit calculation of dF/dt. Assuming  for simplicity that Fdepends only on x and not ?, the chain rule for differentiation yields:  dF _ dE d.v _  dF  dt  d.v d?  dx  so  x(tN + h)^x(tN)+hF  + ~F^  (5.8)  To avoid having to compute dF/dx, let us see if we can find an approximation of the form:  x{tN + h) « x + ahF + bhF{x + chF)  (5.9)  What we have done here is to replace the lr term in (5.8) by a function of a function,  F(x + chF). Three constants, a, b, and c, have been introduced into (5.9), and these must  now be chosen to make (5.9) identical to (5.8). As /; is very small, we may now use Taylor's 