equations with adaptation variables Ai and A2, which operate by increasing the semi- saturation constants a in (2.11), are:  d£,  1 /  100[A) - 3.2F2 "  Fi ±- d?  T \  (120 + AI)2 +  [KI-3.2E2\  dE2 _ 1 /  100[F2 -3.2F,];  d? " r V  2  (120±.4 2) :±[F 2-3.2F 1]- +y  (8.20)  6-± = U-Ai+0Ei)  dt  rA  ^f = U-A2+0E2)  dt  rA  where the small plus sign on the bottom right of the brackets indicates that the entire  bracket evaluates to zero whenever the quantity within is <0. Let r = 20ms, and  rA = 600 ms, thus again recognizing that adaptation is a much slower process that neural  activation. Let K\ = K2 = 150 be the inputs for E\ and E2, and introduce a slight  asymmetry in the initial conditions by setting Fi = 1, £2 =0, A\ =0,z\\d A2 = 0. If you  execute MatLab script WTAadapt.m, with adaptation parameter 0= 1.5, you will find  that the network oscillates as depicted in Fig. 8.11. Note that as Fi adapts, E2 is disin- hibited to the point where it finally switches on, and E\ is then inhibited. This cycle repeats  itself and generates a limit cycle oscillation. This adaptation driven oscillation has a  period of about 3 s, even though neural responses have a 20 ms time constant.  When a system has more than two dimensions, it is impossible to plot the entire state  space or the isoclines, which are now intersecting surfaces rather than lines. We can,  however, frequently gain insight into the dynamics by examining various two-dimensional  projections of the state space. Given the four variables in (8.20), there are six pairs we might  choose to plot. Examination of the various combinations reveals that useful information is  obtained by plotting E\ versus A\ (or, equivalently, F2 versus A2). As shown in Fig. 8.12,  this state space projection clearly reveals the closed limit cycle trajectory.  This limit cycle is more complex than those examined so far. We already know from  Chapter 6 that when there is no adaptation, the asymptotically stable steady states involve  132  Spikes, decisions, and actions  (1)  m  LL  (1)  -*  Q.  W  60  50  40  30  20  10  0  .l\Fl  ' \ E 2  1  1  •J  /  1  /  1  / /  1 J  \  1 ^  \  ' \  X  1  \  \ '  s  \ '  x  \ 1  v  \ 1  *  \l  1  /  /  [\  - 1000  2000  3000  4000  5000  6000  Time (ms)  Fig. 8.11  Limit cycle produced by neural adaptation in a WTA network (8.20).  60  LU  60  Fig. 8.12 Two-dimensional projection of the four-dimensional state space of (8.20) onto the E\,A, plane.  The limit cycle is clearly revealed in this projection.  activity in one neuron and suppression of the other. Weak adaptation does not affect this  scenario and therefore cannot produce a limit cycle. When the adaptation becomes suf- ficiently strong, the firing rate of the active F neuron adapts to the point where it disin- hibits the second £ neuron, which then becomes active and suppresses the first cell, which  is the mechanism for limit cycle genesis here. The Hopf theorem can be applied in this  case, but the computation becomes rather complex. You are encouraged to experiment  using the WTAadapt.m program. For example, try varying 0 in (8.20) to determine the  values at which limit cycles arise and vanish (see Exercises).  There are several well-known perceptual phenomena that are believed to result from  adaptation in competitive neural networks. One phenomenon is the perceptual reversal of  Nonlinear oscillations  133  V  /  /  / 