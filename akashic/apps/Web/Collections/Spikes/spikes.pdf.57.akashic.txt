(7.9)  To find the maximum, set the derivative with respect to fi equal to zero:  —  = -Lisin(n-0i)-L 2sin(fi-02) = 0  (7.10)  dfi  Using the trigonometric substitution:  sin(fi - 0,) = cos(0,) sin(fi) - cos(fi) sin(0,)  (7.11)  98  Spikes, decisions, and actions  and a similar one for the second term in (7.10), we arrive at:  {£, cos(0,) ± L2 cos(02)} sin(fi) - \L] sin(0,) ± L2 sm(92)} cos(fi) = 0  (7.12)  This is easily solved for fi to give the result in (7.8). It is also easy to show that the  maximum value of F n is the length of the resultant vector. This completes a proof of (7.6).  The mechanism permitting WTA networks to perform vector summation should now  be apparent. In a WTA network, the neuron that wins the competition is the one receiving  the maximum stimulus. So, a WTA network with cosine-weighted inputs given by (7.4)  will compute the maximum and solve (7.6) for the vector sum, and it will do so using  parallel processing for any number of input vectors. Sparse population coding makes it  unnecessary for the winning neuron to signal the exact vector sum direction. The program  VectorWTA.m can be used for any number of input vectors simply by activating (i.e.  removing the %) the line asking the number of input vectors.  Vector summation provides an important example of a WTA network that can  accomplish complex and important calculations. All that is required is cosine weighting  of the network inputs according to eqn (7.4) plus competitive inhibition among the  network outputs. Cosine weighting appears to be ubiquitous in the nervous system  (Georgopoulos <??<//., 1986, 1993; Theunissen and Miller, 1991; Lewis and Kristan, 1998).  With respect to motion, we predicted that a WTA competition should occur in MT  (middle temporal) cortex, a primate and human motion analysis area (Wilson et al.,  1992). Definitive evidence for a WTA computation in primate MT was subsequently  provided by Salzman and Newsome (1994). One caveat: the vector summation expla- nation of motion perception requires computation of additional nonlinear or 'second  order' motion signals from the stimulus, an operation for which there is extensive  psychophysical and physiological evidence (Wilson et al., 1992; Wilson, 1994a, 1994b;  Smith, 1994; Albright, 1992; Mareschal and Baker, 1998).  In addition to our work (Wilson et al., 1992; Wilson and Kim, 1994) on motion per- ception, both Touretzky et al. (1993) and Abbott (1994) subsequently explored neural  network models for vector summation using cosine-weighting functions.  7.3  Retinal light adaptation  The retina represents one of the most heavily studied neural networks of the brain. (The  retina develops as an outgrowth of the embryonic neural tube, so it is indeed part of the  brain: the only part visible to the naked eye!) Although many details still remain to be  learned about the retina, there is already a sufficient wealth of material to permit us to  develop a fairly sophisticated understanding of its function. We know, for example, the  major neural cell types and the anatomy of their interconnections. In addition, we know  how the retinal ganglion cells, the cells that provide output to the brain, respond to  various light patterns projected onto the retina. The model to be presented here incor- porates key elements of a model proposed recently to explain retinal function in both light  adaptation and afterimage formation (Wilson, 1997). Excellent summaries of the ana- tomical and physiological literature may be found in several reviews (Dowling, 1987;  Computation by excitatory and inhibitory networks  99  Light  To brain  Fig. 7.7 Schematic diagram of anatomical connections among principal retinal cell types: cones (C).  horizontal cells (H), bipolar cells (B). amacrine cells (A), ganglion cells (G), and interplexiform cells (P).  Excitatory synapses are shown as arrowheads and inhibitory synapses as solid circles.  Wiissle and Boycott, 1991) and a comprehensive recent book by Rodiek (1998). A sum-